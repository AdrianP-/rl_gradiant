{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "pip3 install gym\n",
    "\n",
    "git clone https://github.com/openai/baselines.git\n",
    "cd baselines\n",
    "pip install -e .\n",
    "\n",
    "pip3 install tensorflow    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "observation, reward, done, info = env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i_episode in range(20):\n",
    "    observation = env.reset()\n",
    "    for t in range(100):\n",
    "        env.render()\n",
    "        print(observation)\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import itertools\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "\n",
    "import baselines.common.tf_util as U\n",
    "\n",
    "from baselines import logger\n",
    "from baselines import deepq\n",
    "from baselines.deepq.replay_buffer import ReplayBuffer\n",
    "from baselines.common.schedules import LinearSchedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def model(inpt, num_actions, scope, reuse=False):\n",
    "    \"\"\"This model takes as input an observation and returns values of all actions.\"\"\"\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        out = inpt\n",
    "        out = layers.fully_connected(out, num_outputs=64, activation_fn=tf.nn.tanh)\n",
    "        out = layers.fully_connected(out, num_outputs=num_actions, activation_fn=None)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "act, train, update_target, debug = deepq.build_train(\n",
    "    make_obs_ph=lambda name: U.BatchInput(env.observation_space.shape, name=name),\n",
    "    q_func=model,\n",
    "    num_actions=env.action_space.n,\n",
    "    optimizer=tf.train.AdamOptimizer(learning_rate=5e-4),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the schedule for exploration starting from 1 (every action is random) down to 0.02 (98% of actions are selected according to values predicted by the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exploration = LinearSchedule(schedule_timesteps=10000, initial_p=1.0, final_p=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "episode_rewards = [0.0]\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with U.make_session(8):\n",
    "    U.initialize()\n",
    "    update_target()\n",
    "    for t in itertools.count():\n",
    "        # Take action and update exploration to the newest value\n",
    "        action = act(obs[None], update_eps=exploration.value(t))[0]\n",
    "        new_obs, rew, done, _ = env.step(action)\n",
    "        # Store transition in the replay buffer.\n",
    "        replay_buffer.add(obs, action, rew, new_obs, float(done))\n",
    "        obs = new_obs\n",
    "\n",
    "        episode_rewards[-1] += rew\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "            episode_rewards.append(0)\n",
    "\n",
    "        is_solved = t > 100 and np.mean(episode_rewards[-101:-1]) >= 150\n",
    "        if is_solved:\n",
    "            # Show off the result\n",
    "            env.render()\n",
    "        else:\n",
    "            # Minimize the error in Bellman's equation on a batch sampled from replay buffer.\n",
    "            if t > 1000:\n",
    "                obses_t, actions, rewards, obses_tp1, dones = replay_buffer.sample(32)\n",
    "                train(obses_t, actions, rewards, obses_tp1, dones, np.ones_like(rewards))\n",
    "            # Update target network periodically.\n",
    "            if t % 1000 == 0:\n",
    "                update_target()\n",
    "\n",
    "        if done and len(episode_rewards) % 10 == 0:\n",
    "            print(\"steps: \", t)\n",
    "            print(\"episodes: \", len(episode_rewards))\n",
    "            print(\"mean episode reward: \", round(np.mean(episode_rewards[-101:-1]), 1))\n",
    "            print(\"% time spent exploring: \", int(100 * exploration.value(t)))\n",
    "            print(\"==============================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "https://github.com/AdrianP-/gym_trading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gym import envs\n",
    "envids = [spec.id for spec in envs.registry.all()]\n",
    "for envid in sorted(envids):\n",
    "    print(envid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
