{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(4,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02351904,  0.04558041, -0.01750592,  0.01561808])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "observation, reward, done, info = env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0389074  -0.00936086 -0.03871528  0.02507888]\n",
      "[-0.03909462  0.1862943  -0.03821371 -0.27956339]\n",
      "[-0.03536873 -0.00826228 -0.04380497  0.00082633]\n",
      "[-0.03553398  0.18745963 -0.04378845 -0.30534945]\n",
      "[-0.03178479  0.38317734 -0.04989544 -0.6115143 ]\n",
      "[-0.02412124  0.57895985 -0.06212572 -0.91948575]\n",
      "[-0.01254204  0.77486402 -0.08051544 -1.23102832]\n",
      "[ 0.00295524  0.97092399 -0.105136   -1.5478115 ]\n",
      "[ 0.02237372  1.16713926 -0.13609223 -1.87136232]\n",
      "[ 0.0457165   1.36346101 -0.17351948 -2.20300981]\n",
      "Episode finished after 10 timesteps\n",
      "[ 0.02487422 -0.04051463  0.02235409  0.03932106]\n",
      "[ 0.02406393 -0.23594988  0.02314051  0.33897225]\n",
      "[ 0.01934493 -0.04116472  0.02991995  0.05367543]\n",
      "[ 0.01852164 -0.23670262  0.03099346  0.35564617]\n",
      "[ 0.01378758 -0.43225121  0.03810639  0.65793881]\n",
      "[ 0.00514256 -0.62788227  0.05126516  0.96237293]\n",
      "[-0.00741509 -0.82365424  0.07051262  1.27071044]\n",
      "[-0.02388817 -1.01960194  0.09592683  1.58461513]\n",
      "[-0.04428021 -0.82574278  0.12761913  1.32332254]\n",
      "[-0.06079506 -1.02222485  0.15408558  1.65306713]\n",
      "[-0.08123956 -0.82920142  0.18714693  1.41208443]\n",
      "Episode finished after 11 timesteps\n",
      "[0.02384615 0.0029778  0.03574807 0.01632891]\n",
      "[ 0.0239057  -0.1926381   0.03607465  0.32007299]\n",
      "[ 0.02005294 -0.38825474  0.04247611  0.62391069]\n",
      "[ 0.01228785 -0.19375076  0.05495432  0.3449021 ]\n",
      "[0.00841283 0.00054813 0.06185237 0.07004217]\n",
      "[ 0.00842379  0.1947313   0.06325321 -0.20250258]\n",
      "[ 0.01231842  0.38889429  0.05920316 -0.47458058]\n",
      "[ 0.02009631  0.19298844  0.04971155 -0.1638411 ]\n",
      "[ 0.02395607  0.38736482  0.04643472 -0.44043629]\n",
      "[ 0.03170337  0.19161752  0.037626   -0.13348494]\n",
      "[ 0.03553572  0.38618088  0.0349563  -0.41406399]\n",
      "[ 0.04325934  0.19058134  0.02667502 -0.11056886]\n",
      "[ 0.04707097  0.3853111   0.02446364 -0.39471805]\n",
      "[ 0.05477719  0.58007754  0.01656928 -0.6795887 ]\n",
      "[ 0.06637874  0.38472939  0.00297751 -0.38173555]\n",
      "[ 0.07407333  0.57980894 -0.0046572  -0.67347819]\n",
      "[ 0.08566951  0.77499531 -0.01812677 -0.96762378]\n",
      "[ 0.10116941  0.58012139 -0.03747924 -0.68068991]\n",
      "[ 0.11277184  0.38553949 -0.05109304 -0.40003836]\n",
      "[ 0.12048263  0.58134758 -0.05909381 -0.70838202]\n",
      "[ 0.13210958  0.38709181 -0.07326145 -0.43487046]\n",
      "[ 0.13985142  0.58317031 -0.08195886 -0.74971886]\n",
      "[ 0.15151482  0.77932133 -0.09695324 -1.06702668]\n",
      "[ 0.16710125  0.97558294 -0.11829377 -1.38849617]\n",
      "[ 0.18661291  1.1719635  -0.14606369 -1.71570569]\n",
      "[ 0.21005218  1.36842805 -0.18037781 -2.05005467]\n",
      "Episode finished after 26 timesteps\n",
      "[-0.00010303 -0.0140277  -0.01320998  0.03309776]\n",
      "[-0.00038359  0.18128117 -0.01254802 -0.2637236 ]\n",
      "[ 0.00324204 -0.01365944 -0.01782249  0.02497528]\n",
      "[ 0.00296885  0.1817135  -0.01732299 -0.27327713]\n",
      "[ 0.00660312  0.37707829 -0.02278853 -0.57137305]\n",
      "[ 0.01414468  0.57251227 -0.03421599 -0.87114714]\n",
      "[ 0.02559493  0.76808247 -0.05163893 -1.17438828]\n",
      "[ 0.04095658  0.57366822 -0.0751267  -0.89833075]\n",
      "[ 0.05242994  0.76972365 -0.09309331 -1.21365084]\n",
      "[ 0.06782442  0.57591805 -0.11736633 -0.95153115]\n",
      "[ 0.07934278  0.38255445 -0.13639695 -0.69790767]\n",
      "[ 0.08699387  0.57927761 -0.15035511 -1.03022824]\n",
      "[ 0.09857942  0.77604538 -0.17095967 -1.36608802]\n",
      "[ 0.11410033  0.58342593 -0.19828143 -1.13138853]\n",
      "Episode finished after 14 timesteps\n",
      "[-0.02457741  0.03959311 -0.03630597 -0.01964188]\n",
      "[-0.02378554 -0.15498987 -0.0366988   0.26136864]\n",
      "[-0.02688534  0.04063621 -0.03147143 -0.04265992]\n",
      "[-0.02607262 -0.15402065 -0.03232463  0.2399297 ]\n",
      "[-0.02915303 -0.34866628 -0.02752603  0.52224384]\n",
      "[-0.03612636 -0.5433902  -0.01708116  0.80612743]\n",
      "[-4.69941600e-02 -7.38273896e-01 -9.58609441e-04  1.09338872e+00]\n",
      "[-0.06175964 -0.9333832   0.02090916  1.38577072]\n",
      "[-0.0804273  -0.7385281   0.04862458  1.09969879]\n",
      "[-0.09519786 -0.54407866  0.07061856  0.82265937]\n",
      "[-0.10607944 -0.74009203  0.08707174  1.13669119]\n",
      "[-0.12088128 -0.93623826  0.10980557  1.45536269]\n",
      "[-0.13960604 -1.13252343  0.13891282  1.78023524]\n",
      "[-0.16225651 -0.93921123  0.17451753  1.53376931]\n",
      "[-0.18104074 -0.74656807  0.20519291  1.30024194]\n",
      "Episode finished after 15 timesteps\n",
      "[ 0.02513229 -0.03983621  0.04315978 -0.02414401]\n",
      "[ 0.02433557  0.15464107  0.0426769  -0.30290323]\n",
      "[ 0.02742839 -0.0410623   0.03661883  0.00292774]\n",
      "[ 0.02660714 -0.23668977  0.03667739  0.30693575]\n",
      "[ 0.02187335 -0.04210912  0.0428161   0.02604181]\n",
      "[ 0.02103116 -0.23781811  0.04333694  0.33192033]\n",
      "[ 0.0162748  -0.04333895  0.04997535  0.05321256]\n",
      "[ 0.01540802  0.15103213  0.0510396  -0.2232934 ]\n",
      "[ 0.01842867  0.34538883  0.04657373 -0.49944969]\n",
      "[ 0.02533644  0.53982431  0.03658474 -0.77709891]\n",
      "[ 0.03613293  0.73442455  0.02104276 -1.05805043]\n",
      "[ 5.08214195e-02  9.29261472e-01 -1.18250716e-04 -1.34405501e+00]\n",
      "[ 0.06940665  0.73414101 -0.02699935 -1.05140908]\n",
      "[ 0.08408947  0.92961044 -0.04802753 -1.35244321]\n",
      "[ 0.10268168  1.12530145 -0.0750764  -1.65975591]\n",
      "[ 0.12518771  0.9311305  -0.10827151 -1.39137167]\n",
      "[ 0.14381032  0.73751056 -0.13609895 -1.13441207]\n",
      "[ 0.15856053  0.93412517 -0.15878719 -1.46649647]\n",
      "[ 0.17724303  0.74126344 -0.18811712 -1.22733074]\n",
      "Episode finished after 19 timesteps\n",
      "[ 0.03040241  0.02740559  0.01679074 -0.04256257]\n",
      "[ 0.03095052  0.22228279  0.01593949 -0.32990093]\n",
      "[ 0.03539618  0.41717426  0.00934147 -0.61751497]\n",
      "[ 0.04373967  0.22192307 -0.00300883 -0.32190461]\n",
      "[ 0.04817813  0.41708774 -0.00944692 -0.61553488]\n",
      "[ 0.05651988  0.22209904 -0.02175762 -0.32584224]\n",
      "[ 0.06096186  0.02729352 -0.02827446 -0.04009936]\n",
      "[ 0.06150773  0.22280927 -0.02907645 -0.34156735]\n",
      "[ 0.06596392  0.02811282 -0.0359078  -0.05819339]\n",
      "[ 0.06652617 -0.16647637 -0.03707166  0.22294763]\n",
      "[ 0.06319665 -0.36104939 -0.03261271  0.50371005]\n",
      "[ 0.05597566 -0.16548334 -0.02253851  0.20093053]\n",
      "[ 0.05266599  0.02995358 -0.0185199  -0.09877617]\n",
      "[ 0.05326506 -0.16489811 -0.02049542  0.18800672]\n",
      "[ 0.0499671   0.03051098 -0.01673529 -0.11107059]\n",
      "[ 0.05057732 -0.16436722 -0.0189567   0.17628582]\n",
      "[ 0.04728998 -0.35921282 -0.01543098  0.46292878]\n",
      "[ 0.04010572 -0.55411333 -0.00617241  0.75070821]\n",
      "[ 0.02902345 -0.74914961  0.00884176  1.04144239]\n",
      "[ 0.01404046 -0.94438789  0.0296706   1.33688779]\n",
      "[-0.0048473  -0.74965202  0.05640836  1.05363444]\n",
      "[-0.01984034 -0.94547463  0.07748105  1.36347642]\n",
      "[-0.03874983 -0.75140412  0.10475058  1.09600066]\n",
      "[-0.05377791 -0.94773779  0.12667059  1.41962729]\n",
      "[-0.07273267 -0.75439051  0.15506314  1.16907166]\n",
      "[-0.08782048 -0.56158748  0.17844457  0.92874314]\n",
      "[-0.09905223 -0.3692647   0.19701943  0.69702454]\n",
      "Episode finished after 27 timesteps\n",
      "[ 0.04613322  0.04974013 -0.00830978 -0.0006958 ]\n",
      "[ 0.04712802  0.24498026 -0.0083237  -0.29598894]\n",
      "[ 0.05202763  0.04997796 -0.01424348 -0.00594275]\n",
      "[ 0.05302719  0.24530125 -0.01436233 -0.30308542]\n",
      "[ 0.05793321  0.44062492 -0.02042404 -0.60026308]\n",
      "[ 0.06674571  0.63602655 -0.0324293  -0.89930866]\n",
      "[ 0.07946624  0.83157264 -0.05041548 -1.2020061 ]\n",
      "[ 0.0960977   0.63713759 -0.0744556  -0.92553997]\n",
      "[ 0.10884045  0.83318186 -0.0929664  -1.24066138]\n",
      "[ 0.12550409  1.0293663  -0.11777962 -1.56096004]\n",
      "[ 0.14609141  0.8358341  -0.14899883 -1.30722014]\n",
      "[ 0.16280809  0.64288117 -0.17514323 -1.06464131]\n",
      "[ 0.17566572  0.83983363 -0.19643605 -1.40677967]\n",
      "Episode finished after 13 timesteps\n",
      "[ 0.04655109 -0.02477652  0.01302226 -0.03698903]\n",
      "[ 0.04605556 -0.22008277  0.01228248  0.25977394]\n",
      "[ 0.0416539  -0.41537789  0.01747795  0.55630548]\n",
      "[ 0.03334635 -0.6107408   0.02860406  0.85444335]\n",
      "[ 0.02113153 -0.41602013  0.04569293  0.57089028]\n",
      "[ 0.01281113 -0.22156774  0.05711074  0.29294517]\n",
      "[ 0.00837977 -0.41745548  0.06296964  0.60307916]\n",
      "[ 3.06620842e-05 -6.13399021e-01  7.50312235e-02  9.14912789e-01]\n",
      "[-0.01223732 -0.41936765  0.09332948  0.64672309]\n",
      "[-0.02062467 -0.61565756  0.10626394  0.96727561]\n",
      "[-0.03293782 -0.42211064  0.12560945  0.70977568]\n",
      "[-0.04138004 -0.2289315   0.13980497  0.45912206]\n",
      "[-0.04595867 -0.03603396  0.14898741  0.21356773]\n",
      "[-0.04667934  0.15667869  0.15325876 -0.02865847]\n",
      "[-0.04354577 -0.04027099  0.15268559  0.30818523]\n",
      "[-0.04435119 -0.23720138  0.1588493   0.64485959]\n",
      "[-0.04909522 -0.4341386   0.17174649  0.98305178]\n",
      "[-0.05777799 -0.24168178  0.19140753  0.74885934]\n",
      "[-0.06261163 -0.438856    0.20638471  1.09515014]\n",
      "Episode finished after 19 timesteps\n",
      "[-0.02631456  0.03428883 -0.02622286 -0.0261615 ]\n",
      "[-0.02562878  0.22977683 -0.02674609 -0.32700132]\n",
      "[-0.02103325  0.42526916 -0.03328612 -0.62799735]\n",
      "[-0.01252786  0.2306272  -0.04584606 -0.34598044]\n",
      "[-0.00791532  0.42637029 -0.05276567 -0.65276059]\n",
      "[ 0.00061209  0.2320213  -0.06582089 -0.37714876]\n",
      "[ 0.00525251  0.03789292 -0.07336386 -0.10592411]\n",
      "[ 0.00601037  0.23398536 -0.07548234 -0.42082118]\n",
      "[ 0.01069008  0.04000951 -0.08389877 -0.15285666]\n",
      "[ 0.01149027  0.23622632 -0.0869559  -0.47078391]\n",
      "[ 0.01621479  0.43246199 -0.09637158 -0.7895581 ]\n",
      "[ 0.02486403  0.2387863  -0.11216274 -0.52868199]\n",
      "[ 0.02963976  0.04540621 -0.12273638 -0.2733413 ]\n",
      "[ 0.03054788  0.24204617 -0.12820321 -0.60207632]\n",
      "[ 0.03538881  0.04892852 -0.14024473 -0.35236575]\n",
      "[ 0.03636738  0.2457372  -0.14729205 -0.68577723]\n",
      "[ 0.04128212  0.05293373 -0.16100759 -0.44285095]\n",
      "[ 0.0423408   0.24992395 -0.16986461 -0.78164653]\n",
      "[ 0.04733928  0.05749313 -0.18549754 -0.54685498]\n",
      "[ 0.04848914  0.2546702  -0.19643464 -0.89177065]\n",
      "Episode finished after 20 timesteps\n",
      "[ 0.01429195  0.0023812   0.01430249 -0.0015125 ]\n",
      "[ 0.01433958  0.19729514  0.01427224 -0.28964872]\n",
      "[0.01828548 0.00197262 0.00847926 0.00750113]\n",
      "[ 0.01832493 -0.19326991  0.00862929  0.30284726]\n",
      "[ 0.01445954 -0.38851377  0.01468623  0.59823913]\n",
      "[ 0.00668926 -0.19360036  0.02665101  0.31021814]\n",
      "[ 0.00281725 -0.38909169  0.03285538  0.61118547]\n",
      "[-0.00496458 -0.19444399  0.04507909  0.32902923]\n",
      "[-0.00885346 -0.39017772  0.05165967  0.63558033]\n",
      "[-0.01665702 -0.1958129   0.06437128  0.35960323]\n",
      "[-0.02057327 -0.39178803  0.07156334  0.67186918]\n",
      "[-0.02840903 -0.58782797  0.08500073  0.9861981 ]\n",
      "[-0.04016559 -0.78397909  0.10472469  1.30432264]\n",
      "[-0.05584517 -0.98026155  0.13081114  1.6278661 ]\n",
      "[-0.07545041 -1.17665647  0.16336846  1.9582905 ]\n",
      "[-0.09898354 -0.98360116  0.20253427  1.72038029]\n",
      "Episode finished after 16 timesteps\n",
      "[ 0.01920567  0.01754983  0.00346631 -0.01019114]\n",
      "[ 0.01955666 -0.17762166  0.00326248  0.28358343]\n",
      "[ 0.01600423  0.0174536   0.00893415 -0.00806875]\n",
      "[ 0.0163533  -0.17779533  0.00877278  0.28741957]\n",
      "[ 0.0127974  -0.37304128  0.01452117  0.58285639]\n",
      "[ 0.00533657 -0.56836363  0.0261783   0.8800781 ]\n",
      "[-0.0060307  -0.37360693  0.04377986  0.59573868]\n",
      "[-0.01350284 -0.56931336  0.05569463  0.90188422]\n",
      "[-0.02488911 -0.76514383  0.07373232  1.21154021]\n",
      "[-0.04019199 -0.57104706  0.09796312  0.94284376]\n",
      "[-0.05161293 -0.76734275  0.11682     1.26463068]\n",
      "[-0.06695978 -0.5738914   0.14211261  1.01069844]\n",
      "[-0.07843761 -0.77059425  0.16232658  1.34441887]\n",
      "[-0.09384949 -0.96734251  0.18921496  1.68317628]\n",
      "Episode finished after 14 timesteps\n",
      "[-0.04816194  0.01017363  0.03402362 -0.02611951]\n",
      "[-0.04795846  0.20479155  0.03350123 -0.30787643]\n",
      "[-0.04386263  0.00920866  0.0273437  -0.00481918]\n",
      "[-0.04367846 -0.18629455  0.02724732  0.29636406]\n",
      "[-0.04740435 -0.38179412  0.0331746   0.59751428]\n",
      "[-0.05504023 -0.18715171  0.04512488  0.31546285]\n",
      "[-0.05878327  0.00729939  0.05143414  0.03734533]\n",
      "[-0.05863728 -0.18852096  0.05218105  0.34580239]\n",
      "[-0.0624077  -0.38434484  0.0590971   0.65447267]\n",
      "[-0.07009459 -0.58023768  0.07218655  0.96516339]\n",
      "[-0.08169935 -0.77625114  0.09148982  1.27962228]\n",
      "[-0.09722437 -0.97241213  0.11708226  1.59949366]\n",
      "[-0.11667261 -0.77885569  0.14907214  1.34549032]\n",
      "[-0.13224973 -0.97550431  0.17598194  1.68085686]\n",
      "Episode finished after 14 timesteps\n",
      "[0.04411543 0.0433279  0.04834575 0.00603027]\n",
      "[ 0.04498199 -0.15245286  0.04846635  0.31356639]\n",
      "[ 0.04193293 -0.34823053  0.05473768  0.62113158]\n",
      "[ 0.03496832 -0.54407244  0.06716031  0.93053932]\n",
      "[ 0.02408687 -0.34991807  0.0857711   0.6596946 ]\n",
      "[ 0.01708851 -0.15608788  0.09896499  0.39520393]\n",
      "[0.01396675 0.03750079 0.10686907 0.13529001]\n",
      "[ 0.01471677 -0.1589767   0.10957487  0.45968461]\n",
      "[ 0.01153723 -0.3554631   0.11876856  0.78479719]\n",
      "[ 0.00442797 -0.16215587  0.1344645   0.53171497]\n",
      "[0.00118485 0.030844   0.1450988  0.28424194]\n",
      "[0.00180173 0.22363059 0.15078364 0.04061038]\n",
      "[ 0.00627434  0.41630484  0.15159585 -0.20095918]\n",
      "[0.01460044 0.21937651 0.14757667 0.13544423]\n",
      "[0.01898797 0.02248282 0.15028555 0.47080473]\n",
      "[ 0.01943763 -0.17440683  0.15970165  0.80682811]\n",
      "[0.01594949 0.0182081  0.17583821 0.56833528]\n",
      "[0.01631365 0.21048452 0.18720491 0.33579837]\n",
      "[0.02052334 0.01326041 0.19392088 0.68118311]\n",
      "[0.02078855 0.20523603 0.20754454 0.45527328]\n",
      "Episode finished after 20 timesteps\n",
      "[-0.03948418 -0.00342143 -0.01155311  0.04322621]\n",
      "[-0.0395526  -0.19837583 -0.01068858  0.33224174]\n",
      "[-0.04352012 -0.00310338 -0.00404375  0.03620742]\n",
      "[-0.04358219 -0.19816711 -0.0033196   0.32761177]\n",
      "[-0.04754553 -0.00299806  0.00323263  0.03388384]\n",
      "[-0.04760549  0.19207739  0.00391031 -0.25777741]\n",
      "[-0.04376394  0.38714329 -0.00124524 -0.5492244 ]\n",
      "[-0.03602108  0.58228272 -0.01222973 -0.84229941]\n",
      "[-0.02437542  0.38732981 -0.02907571 -0.55348737]\n",
      "[-0.01662883  0.19262796 -0.04014546 -0.27010509]\n",
      "[-0.01277627 -0.00189879 -0.04554756  0.00965034]\n",
      "[-0.01281424  0.19384578 -0.04535456 -0.29704835]\n",
      "[-0.00893733  0.38958394 -0.05129552 -0.60368334]\n",
      "[-0.00114565  0.58538436 -0.06336919 -0.91207193]\n",
      "[ 0.01056204  0.39117438 -0.08161063 -0.63995975]\n",
      "[ 0.01838553  0.5873336  -0.09440982 -0.95718616]\n",
      "[ 0.0301322   0.78358962 -0.11355355 -1.27797448]\n",
      "[ 0.04580399  0.9799611  -0.13911304 -1.60394772]\n",
      "[ 0.06540321  1.17642805 -0.17119199 -1.9365686 ]\n",
      "Episode finished after 19 timesteps\n",
      "[-0.04221862 -0.00354073 -0.03256542 -0.00643448]\n",
      "[-0.04228943 -0.19818088 -0.03269411  0.2757984 ]\n",
      "[-0.04625305 -0.0026081  -0.02717814 -0.02701435]\n",
      "[-0.04630521  0.19289285 -0.02771843 -0.32814687]\n",
      "[-0.04244736 -0.00182375 -0.03428137 -0.04433234]\n",
      "[-0.04248383 -0.19643778 -0.03516801  0.23734044]\n",
      "[-0.04641259 -0.39104011 -0.03042121  0.51872618]\n",
      "[-0.05423339 -0.19550337 -0.02004668  0.21661432]\n",
      "[-0.05814346 -0.00010066 -0.0157144  -0.08232419]\n",
      "[-0.05814547 -0.19499386 -0.01736088  0.20535968]\n",
      "[-0.06204535  0.000372   -0.01325369 -0.09274879]\n",
      "[-0.06203791  0.19568138 -0.01510866 -0.3895836 ]\n",
      "[-0.05812428  0.39101448 -0.02290033 -0.68699159]\n",
      "[-0.05030399  0.5864467  -0.03664016 -0.98679516]\n",
      "[-0.03857506  0.78203965 -0.05637607 -1.29075747]\n",
      "[-0.02293426  0.58767807 -0.08219122 -1.01624406]\n",
      "[-0.0111807   0.39374248 -0.1025161  -0.75045973]\n",
      "[-0.00330585  0.20017259 -0.11752529 -0.49171627]\n",
      "[ 6.97598811e-04  3.96739140e-01 -1.27359618e-01 -8.19005130e-01]\n",
      "[ 0.00863238  0.20356893 -0.14373972 -0.56893922]\n",
      "[ 0.01270376  0.40038331 -0.15511851 -0.90323033]\n",
      "[ 0.02071143  0.2076641  -0.17318311 -0.66304458]\n",
      "[ 0.02486471  0.40471833 -0.186444   -1.00486611]\n",
      "[ 0.03295908  0.60177458 -0.20654133 -1.34982541]\n",
      "Episode finished after 24 timesteps\n",
      "[ 0.0487528   0.04358619 -0.04624512 -0.02507703]\n",
      "[ 0.04962452 -0.1508431  -0.04674666  0.25266389]\n",
      "[ 0.04660766 -0.34526746 -0.04169339  0.53024333]\n",
      "[ 0.03970231 -0.14958457 -0.03108852  0.22471977]\n",
      "[ 0.03671062 -0.34424872 -0.02659412  0.50743635]\n",
      "[ 0.02982565 -0.53898606 -0.0164454   0.79162131]\n",
      "[ 0.01904592 -0.34364222 -0.00061297  0.49381045]\n",
      "[ 0.01217308 -0.14851163  0.00926324  0.2009344 ]\n",
      "[ 0.00920285  0.04647663  0.01328193 -0.08881207]\n",
      "[ 0.01013238  0.2414057   0.01150569 -0.37727513]\n",
      "[ 0.01496049  0.04612225  0.00396018 -0.08098674]\n",
      "[ 0.01588294  0.24118721  0.00234045 -0.37241759]\n",
      "[ 0.02070668  0.04603208 -0.0051079  -0.07899762]\n",
      "[ 0.02162732 -0.14901627 -0.00668786  0.21206938]\n",
      "[ 0.018647   -0.34404197 -0.00244647  0.50263517]\n",
      "[ 0.01176616 -0.53912935  0.00760623  0.79454611]\n",
      "[ 0.00098357 -0.34411262  0.02349716  0.50426569]\n",
      "[-0.00589868 -0.14932957  0.03358247  0.21907929]\n",
      "[-0.00888527  0.04529664  0.03796406 -0.06282403]\n",
      "[-0.00797934 -0.15034848  0.03670758  0.24159095]\n",
      "[-0.01098631  0.04423042  0.04153939 -0.03929118]\n",
      "[-0.0101017  -0.15146183  0.04075357  0.26620304]\n",
      "[-0.01313094 -0.34714101  0.04607763  0.57145634]\n",
      "[-0.02007376 -0.15269446  0.05750676  0.29363812]\n",
      "[-0.02312765  0.04156246  0.06337952  0.01963209]\n",
      "[-0.0222964   0.23572092  0.06377216 -0.25239975]\n",
      "[-0.01758198  0.42987702  0.05872417 -0.52430559]\n",
      "[-0.00898444  0.6241255   0.04823806 -0.79792106]\n",
      "[ 0.00349807  0.81855359  0.03227963 -1.07504747]\n",
      "[ 0.01986914  0.62302032  0.01077869 -0.77241178]\n",
      "[ 0.03232955  0.42775173 -0.00466955 -0.47635704]\n",
      "[ 0.04088459  0.62293931 -0.01419669 -0.77050805]\n",
      "[ 0.05334337  0.81825373 -0.02960685 -1.06762385]\n",
      "[ 0.06970845  1.01375458 -0.05095933 -1.36944976]\n",
      "[ 0.08998354  1.20947575 -0.07834832 -1.67762595]\n",
      "[ 0.11417305  1.405414   -0.11190084 -1.99364257]\n",
      "[ 0.14228133  1.60151548 -0.15177369 -2.31878528]\n",
      "[ 0.17431164  1.79765964 -0.1981494  -2.65406971]\n",
      "Episode finished after 38 timesteps\n",
      "[0.0059142  0.04440484 0.03332058 0.02547286]\n",
      "[ 0.0068023   0.23903349  0.03383004 -0.25651363]\n",
      "[ 0.01158297  0.43365652  0.02869977 -0.538337  ]\n",
      "[ 0.0202561   0.23814309  0.01793303 -0.23675105]\n",
      "[0.02501896 0.04276959 0.01319801 0.06153411]\n",
      "[ 0.02587436 -0.15253908  0.01442869  0.3583517 ]\n",
      "[ 0.02282357 -0.34786315  0.02159572  0.6555492 ]\n",
      "[ 0.01586631 -0.15304841  0.03470671  0.36974388]\n",
      "[0.01280534 0.04156368 0.04210159 0.08820327]\n",
      "[ 0.01363662  0.23605765  0.04386565 -0.19090496]\n",
      "[0.01835777 0.04033651 0.04004755 0.11528671]\n",
      "[ 0.0191645   0.23486244  0.04235329 -0.16449739]\n",
      "[ 0.02386175  0.42935331  0.03906334 -0.44352394]\n",
      "[ 0.03244881  0.62390139  0.03019286 -0.72364174]\n",
      "[ 0.04492684  0.81859304  0.01572002 -1.00667075]\n",
      "[ 0.0612987   1.01350159 -0.00441339 -1.29437597]\n",
      "[ 0.08156873  1.20867935 -0.03030091 -1.58843728]\n",
      "[ 0.10574232  1.40414789 -0.06206966 -1.8904126 ]\n",
      "[ 0.13382528  1.20975228 -0.09987791 -1.61761748]\n",
      "[ 0.15802032  1.01593967 -0.13223026 -1.35766271]\n",
      "[ 0.17833912  0.82270076 -0.15938351 -1.10909722]\n",
      "[ 0.19479313  0.6299908  -0.18156546 -0.8703567 ]\n",
      "[ 0.20739295  0.82705621 -0.19897259 -1.21418328]\n",
      "Episode finished after 23 timesteps\n",
      "[-0.0349803   0.00360624  0.0350941   0.00465053]\n",
      "[-0.03490817 -0.19200098  0.03518711  0.30819624]\n",
      "[-0.03874819  0.00260239  0.04135104  0.02681475]\n",
      "[-0.03869615 -0.19308742  0.04188733  0.33225228]\n",
      "[-0.04255789 -0.38877978  0.04853238  0.63784462]\n",
      "[-0.05033349 -0.19436698  0.06128927  0.36083171]\n",
      "[-0.05422083 -0.39030414  0.0685059   0.67219339]\n",
      "[-0.06202691 -0.19619797  0.08194977  0.40184206]\n",
      "[-0.06595087 -0.39238086  0.08998661  0.71919509]\n",
      "[-0.07379849 -0.19861165  0.10437051  0.45613764]\n",
      "[-0.07777072 -0.39504243  0.11349327  0.7798097 ]\n",
      "[-0.08567157 -0.59152652  0.12908946  1.10593517]\n",
      "[-0.0975021  -0.78808735  0.15120816  1.43617018]\n",
      "[-0.11326385 -0.98471538  0.17993157  1.77203286]\n",
      "Episode finished after 14 timesteps\n",
      "[ 0.00810983  0.01095346 -0.03474462  0.01968016]\n",
      "[ 0.0083289   0.20655601 -0.03435101 -0.28375946]\n",
      "[ 0.01246002  0.40215064 -0.0400262  -0.58707553]\n",
      "[ 0.02050303  0.20761145 -0.05176771 -0.30726502]\n",
      "[ 0.02465526  0.40343137 -0.05791301 -0.61581432]\n",
      "[ 0.03272389  0.20916433 -0.0702293  -0.34191919]\n",
      "[ 0.03690718  0.40521152 -0.07706768 -0.65589553]\n",
      "[ 0.04501141  0.21124228 -0.09018559 -0.38844076]\n",
      "[ 0.04923625  0.01750845 -0.09795441 -0.12549964]\n",
      "[ 0.04958642 -0.17608368 -0.1004644   0.13474396]\n",
      "[ 0.04606475 -0.36963382 -0.09776952  0.39411838]\n",
      "[ 0.03867207 -0.17327033 -0.08988715  0.07228131]\n",
      "[ 0.03520667  0.02301762 -0.08844153 -0.24735319]\n",
      "[ 0.03566702  0.21928403 -0.09338859 -0.56657011]\n",
      "[ 0.0400527   0.02558768 -0.10471999 -0.304708  ]\n",
      "[ 0.04056445  0.22203399 -0.11081415 -0.62849467]\n",
      "[ 0.04500513  0.02861876 -0.12338405 -0.37266404]\n",
      "[ 0.04557751 -0.16455417 -0.13083733 -0.12129038]\n",
      "[ 0.04228642  0.03217595 -0.13326314 -0.45221991]\n",
      "[ 0.04292994  0.2289059  -0.14230753 -0.78376191]\n",
      "[ 0.04750806  0.42566671 -0.15798277 -1.11761699]\n",
      "[ 0.0560214   0.2329303  -0.18033511 -0.87836666]\n",
      "[ 0.06068     0.42998397 -0.19790245 -1.22188554]\n",
      "Episode finished after 23 timesteps\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(20):\n",
    "    observation = env.reset()\n",
    "    for t in range(100):\n",
    "        env.render()\n",
    "        print(observation)\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import itertools\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "\n",
    "import baselines.common.tf_util as U\n",
    "\n",
    "from baselines import logger\n",
    "from baselines import deepq\n",
    "from baselines.deepq.replay_buffer import ReplayBuffer\n",
    "from baselines.common.schedules import LinearSchedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def model(inpt, num_actions, scope, reuse=False):\n",
    "    \"\"\"This model takes as input an observation and returns values of all actions.\"\"\"\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        out = inpt\n",
    "        out = layers.fully_connected(out, num_outputs=64, activation_fn=tf.nn.tanh)\n",
    "        out = layers.fully_connected(out, num_outputs=num_actions, activation_fn=None)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    }
   ],
   "source": [
    "act, train, update_target, debug = deepq.build_train(\n",
    "    make_obs_ph=lambda name: U.BatchInput(env.observation_space.shape, name=name),\n",
    "    q_func=model,\n",
    "    num_actions=env.action_space.n,\n",
    "    optimizer=tf.train.AdamOptimizer(learning_rate=5e-4),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the schedule for exploration starting from 1 (every action is random) down to 0.02 (98% of actions are selected according to values predicted by the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exploration = LinearSchedule(schedule_timesteps=10000, initial_p=1.0, final_p=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "episode_rewards = [0.0]\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/local/lib/python3.5/dist-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps:  189\n",
      "episodes:  10\n",
      "mean episode reward:  21.1\n",
      "% time spent exploring:  98\n",
      "==============================\n",
      "steps:  381\n",
      "episodes:  20\n",
      "mean episode reward:  20.1\n",
      "% time spent exploring:  96\n",
      "==============================\n",
      "steps:  568\n",
      "episodes:  30\n",
      "mean episode reward:  19.6\n",
      "% time spent exploring:  94\n",
      "==============================\n",
      "steps:  769\n",
      "episodes:  40\n",
      "mean episode reward:  19.7\n",
      "% time spent exploring:  92\n",
      "==============================\n",
      "steps:  948\n",
      "episodes:  50\n",
      "mean episode reward:  19.4\n",
      "% time spent exploring:  90\n",
      "==============================\n",
      "steps:  1159\n",
      "episodes:  60\n",
      "mean episode reward:  19.7\n",
      "% time spent exploring:  88\n",
      "==============================\n",
      "steps:  1352\n",
      "episodes:  70\n",
      "mean episode reward:  19.6\n",
      "% time spent exploring:  86\n",
      "==============================\n",
      "steps:  1499\n",
      "episodes:  80\n",
      "mean episode reward:  19.0\n",
      "% time spent exploring:  85\n",
      "==============================\n",
      "steps:  1672\n",
      "episodes:  90\n",
      "mean episode reward:  18.8\n",
      "% time spent exploring:  83\n",
      "==============================\n",
      "steps:  1860\n",
      "episodes:  100\n",
      "mean episode reward:  18.8\n",
      "% time spent exploring:  81\n",
      "==============================\n",
      "steps:  2033\n",
      "episodes:  110\n",
      "mean episode reward:  18.4\n",
      "% time spent exploring:  80\n",
      "==============================\n",
      "steps:  2237\n",
      "episodes:  120\n",
      "mean episode reward:  18.6\n",
      "% time spent exploring:  78\n",
      "==============================\n",
      "steps:  2407\n",
      "episodes:  130\n",
      "mean episode reward:  18.4\n",
      "% time spent exploring:  76\n",
      "==============================\n",
      "steps:  2549\n",
      "episodes:  140\n",
      "mean episode reward:  17.8\n",
      "% time spent exploring:  75\n",
      "==============================\n",
      "steps:  2755\n",
      "episodes:  150\n",
      "mean episode reward:  18.1\n",
      "% time spent exploring:  73\n",
      "==============================\n",
      "steps:  2940\n",
      "episodes:  160\n",
      "mean episode reward:  17.8\n",
      "% time spent exploring:  71\n",
      "==============================\n",
      "steps:  3123\n",
      "episodes:  170\n",
      "mean episode reward:  17.7\n",
      "% time spent exploring:  69\n",
      "==============================\n",
      "steps:  3279\n",
      "episodes:  180\n",
      "mean episode reward:  17.8\n",
      "% time spent exploring:  67\n",
      "==============================\n",
      "steps:  3445\n",
      "episodes:  190\n",
      "mean episode reward:  17.7\n",
      "% time spent exploring:  66\n",
      "==============================\n",
      "steps:  3592\n",
      "episodes:  200\n",
      "mean episode reward:  17.3\n",
      "% time spent exploring:  64\n",
      "==============================\n",
      "steps:  3799\n",
      "episodes:  210\n",
      "mean episode reward:  17.7\n",
      "% time spent exploring:  62\n",
      "==============================\n",
      "steps:  4000\n",
      "episodes:  220\n",
      "mean episode reward:  17.6\n",
      "% time spent exploring:  60\n",
      "==============================\n",
      "steps:  4182\n",
      "episodes:  230\n",
      "mean episode reward:  17.8\n",
      "% time spent exploring:  59\n",
      "==============================\n",
      "steps:  4359\n",
      "episodes:  240\n",
      "mean episode reward:  18.1\n",
      "% time spent exploring:  57\n",
      "==============================\n",
      "steps:  4523\n",
      "episodes:  250\n",
      "mean episode reward:  17.7\n",
      "% time spent exploring:  55\n",
      "==============================\n",
      "steps:  4712\n",
      "episodes:  260\n",
      "mean episode reward:  17.7\n",
      "% time spent exploring:  53\n",
      "==============================\n",
      "steps:  4907\n",
      "episodes:  270\n",
      "mean episode reward:  17.8\n",
      "% time spent exploring:  51\n",
      "==============================\n",
      "steps:  5127\n",
      "episodes:  280\n",
      "mean episode reward:  18.5\n",
      "% time spent exploring:  49\n",
      "==============================\n",
      "steps:  5422\n",
      "episodes:  290\n",
      "mean episode reward:  19.8\n",
      "% time spent exploring:  46\n",
      "==============================\n",
      "steps:  5654\n",
      "episodes:  300\n",
      "mean episode reward:  20.6\n",
      "% time spent exploring:  44\n",
      "==============================\n",
      "steps:  5931\n",
      "episodes:  310\n",
      "mean episode reward:  21.3\n",
      "% time spent exploring:  41\n",
      "==============================\n",
      "steps:  6189\n",
      "episodes:  320\n",
      "mean episode reward:  21.9\n",
      "% time spent exploring:  39\n",
      "==============================\n",
      "steps:  6498\n",
      "episodes:  330\n",
      "mean episode reward:  23.2\n",
      "% time spent exploring:  36\n",
      "==============================\n",
      "steps:  6839\n",
      "episodes:  340\n",
      "mean episode reward:  24.8\n",
      "% time spent exploring:  32\n",
      "==============================\n",
      "steps:  7141\n",
      "episodes:  350\n",
      "mean episode reward:  26.2\n",
      "% time spent exploring:  30\n",
      "==============================\n",
      "steps:  7528\n",
      "episodes:  360\n",
      "mean episode reward:  28.2\n",
      "% time spent exploring:  26\n",
      "==============================\n",
      "steps:  7887\n",
      "episodes:  370\n",
      "mean episode reward:  29.8\n",
      "% time spent exploring:  22\n",
      "==============================\n",
      "steps:  8319\n",
      "episodes:  380\n",
      "mean episode reward:  31.9\n",
      "% time spent exploring:  18\n",
      "==============================\n",
      "steps:  8865\n",
      "episodes:  390\n",
      "mean episode reward:  34.4\n",
      "% time spent exploring:  13\n",
      "==============================\n",
      "steps:  9521\n",
      "episodes:  400\n",
      "mean episode reward:  38.7\n",
      "% time spent exploring:  6\n",
      "==============================\n",
      "steps:  10385\n",
      "episodes:  410\n",
      "mean episode reward:  44.5\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  11217\n",
      "episodes:  420\n",
      "mean episode reward:  50.3\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  12221\n",
      "episodes:  430\n",
      "mean episode reward:  57.2\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  13284\n",
      "episodes:  440\n",
      "mean episode reward:  64.4\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  14348\n",
      "episodes:  450\n",
      "mean episode reward:  72.1\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  15363\n",
      "episodes:  460\n",
      "mean episode reward:  78.4\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  16557\n",
      "episodes:  470\n",
      "mean episode reward:  86.7\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  17756\n",
      "episodes:  480\n",
      "mean episode reward:  94.4\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  18718\n",
      "episodes:  490\n",
      "mean episode reward:  98.5\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  19589\n",
      "episodes:  500\n",
      "mean episode reward:  100.7\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  20521\n",
      "episodes:  510\n",
      "mean episode reward:  101.4\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  21658\n",
      "episodes:  520\n",
      "mean episode reward:  104.4\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  22877\n",
      "episodes:  530\n",
      "mean episode reward:  106.6\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  23968\n",
      "episodes:  540\n",
      "mean episode reward:  106.8\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  25066\n",
      "episodes:  550\n",
      "mean episode reward:  107.2\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  26223\n",
      "episodes:  560\n",
      "mean episode reward:  108.6\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  27491\n",
      "episodes:  570\n",
      "mean episode reward:  109.3\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  28827\n",
      "episodes:  580\n",
      "mean episode reward:  110.7\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  30229\n",
      "episodes:  590\n",
      "mean episode reward:  115.1\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  31813\n",
      "episodes:  600\n",
      "mean episode reward:  122.2\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  33589\n",
      "episodes:  610\n",
      "mean episode reward:  130.7\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  35527\n",
      "episodes:  620\n",
      "mean episode reward:  138.7\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  37438\n",
      "episodes:  630\n",
      "mean episode reward:  145.6\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  39438\n",
      "episodes:  640\n",
      "mean episode reward:  154.7\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  41312\n",
      "episodes:  650\n",
      "mean episode reward:  162.5\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  43312\n",
      "episodes:  660\n",
      "mean episode reward:  170.9\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  45312\n",
      "episodes:  670\n",
      "mean episode reward:  178.2\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  47312\n",
      "episodes:  680\n",
      "mean episode reward:  184.8\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  49269\n",
      "episodes:  690\n",
      "mean episode reward:  190.4\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  51200\n",
      "episodes:  700\n",
      "mean episode reward:  193.9\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  53071\n",
      "episodes:  710\n",
      "mean episode reward:  194.8\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  55060\n",
      "episodes:  720\n",
      "mean episode reward:  195.3\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  56976\n",
      "episodes:  730\n",
      "mean episode reward:  195.4\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  58976\n",
      "episodes:  740\n",
      "mean episode reward:  195.4\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  60835\n",
      "episodes:  750\n",
      "mean episode reward:  195.2\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  62794\n",
      "episodes:  760\n",
      "mean episode reward:  194.8\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  64112\n",
      "episodes:  770\n",
      "mean episode reward:  188.0\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  65675\n",
      "episodes:  780\n",
      "mean episode reward:  183.6\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  67244\n",
      "episodes:  790\n",
      "mean episode reward:  179.8\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  69101\n",
      "episodes:  800\n",
      "mean episode reward:  179.0\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  70690\n",
      "episodes:  810\n",
      "mean episode reward:  176.2\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  72551\n",
      "episodes:  820\n",
      "mean episode reward:  174.9\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  74516\n",
      "episodes:  830\n",
      "mean episode reward:  175.4\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  76516\n",
      "episodes:  840\n",
      "mean episode reward:  175.4\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  78516\n",
      "episodes:  850\n",
      "mean episode reward:  176.8\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  80516\n",
      "episodes:  860\n",
      "mean episode reward:  177.2\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  82516\n",
      "episodes:  870\n",
      "mean episode reward:  184.0\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  84516\n",
      "episodes:  880\n",
      "mean episode reward:  188.4\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  86516\n",
      "episodes:  890\n",
      "mean episode reward:  192.7\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  88516\n",
      "episodes:  900\n",
      "mean episode reward:  194.2\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  90516\n",
      "episodes:  910\n",
      "mean episode reward:  198.3\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  92516\n",
      "episodes:  920\n",
      "mean episode reward:  199.6\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  94516\n",
      "episodes:  930\n",
      "mean episode reward:  200.0\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  96516\n",
      "episodes:  940\n",
      "mean episode reward:  200.0\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  98516\n",
      "episodes:  950\n",
      "mean episode reward:  200.0\n",
      "% time spent exploring:  2\n",
      "==============================\n",
      "steps:  100516\n",
      "episodes:  960\n",
      "mean episode reward:  200.0\n",
      "% time spent exploring:  2\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "with U.make_session(8):\n",
    "    U.initialize()\n",
    "    update_target()\n",
    "    for t in itertools.count():\n",
    "        # Take action and update exploration to the newest value\n",
    "        action = act(obs[None], update_eps=exploration.value(t))[0]\n",
    "        new_obs, rew, done, _ = env.step(action)\n",
    "        # Store transition in the replay buffer.\n",
    "        replay_buffer.add(obs, action, rew, new_obs, float(done))\n",
    "        obs = new_obs\n",
    "\n",
    "        episode_rewards[-1] += rew\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "            episode_rewards.append(0)\n",
    "\n",
    "        is_solved = t > 100 and np.mean(episode_rewards[-101:-1]) >= 170\n",
    "        if is_solved:\n",
    "            # Show off the result\n",
    "            env.render()\n",
    "        else:\n",
    "            # Minimize the error in Bellman's equation on a batch sampled from replay buffer.\n",
    "            if t > 1000:\n",
    "                obses_t, actions, rewards, obses_tp1, dones = replay_buffer.sample(32)\n",
    "                train(obses_t, actions, rewards, obses_tp1, dones, np.ones_like(rewards))\n",
    "            # Update target network periodically.\n",
    "            if t % 1000 == 0:\n",
    "                update_target()\n",
    "\n",
    "        if done and len(episode_rewards) % 10 == 0:\n",
    "            print(\"steps: \", t)\n",
    "            print(\"episodes: \", len(episode_rewards))\n",
    "            print(\"mean episode reward: \", round(np.mean(episode_rewards[-101:-1]), 1))\n",
    "            print(\"% time spent exploring: \", int(100 * exploration.value(t)))\n",
    "            print(\"==============================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
